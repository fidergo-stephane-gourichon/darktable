<sect1 status="draft" id="darktable_and_processing_order">

  <title>darktable and processing order</title>

  <sect2>

    <title>Introduction</title>

  <indexterm>
    <primary>module processing order</primary>
  </indexterm>
    
    <para>
      Users are sometimes puzzled by the various ways that the different
      steps of image processing are displayed in the darkroom view:
      <itemizedlist>
	
	<listitem><para>
	  on the right, the module panel shows modules, in a fixed order
	</para></listitem>
	
	<listitem><para>
	  on the left, the history stack panel names modules in the order they were enabled or
	  disabled
	</para></listitem>
	
	<listitem><para> 
	  on the left, the snapshots panel shows module names, which are, in the order snapshots were
	  created, names of modules enabled, disabled or adjusted in some way.
	</para></listitem>
	
      </itemizedlist>
    </para>
    
    <para>
      This does not make obvious in what order the various processing operations is done in darktable.
    </para>
    
  </sect2>

  <sect2>

    <title>Actual processing order</title>
    
    <para>
      Actually, darktable processed modules in a fixed order, the order which is seen in the module
      panel, from bottom to top.
    </para>

    <para>
      For example, when working on a raw file, the history stack on the left might say that you
      first enabled <link linkend="denoise_bilateral"><emphasis>bilateral
      filtering</emphasis></link>, then disabled <link linkend="base_curve"><emphasis>base
      curve</emphasis></link>, then adjusted <link linkend="whitebalance"><emphasis>white
      balance</emphasis></link>.  But at any time, the processing took the RAW image (not even
      demosaiced), adjusted <link linkend="whitebalance"><emphasis>white balance</emphasis></link>
      on it, then <link linkend="demosaic"><emphasis>demosaic</emphasis></link>, then <link
      linkend="base_curve"><emphasis>base curve</emphasis></link> (if enabled), then <link
      linkend="denoise_bilateral"><emphasis>bilateral filtering</emphasis></link> (if enabled), as
      shown bottom to top on the right panel.
    </para>

  </sect2>

  <sect2>

    <title>Why this order</title>

    <para>
      This strict order may not be obvious at first. There are some reasons 
      easy to summarize, and many others not easy.
    </para>

    <para>
      First, one should know that there are many different ways to represent an image: components
      (like RGB or Lab), but also other aspects of colorspace like linear or pre-applied gamma.
    </para>

    <para>
      A linear colorspace is necessary for several things as simple as adjusting the <link
      linkend="exposure"><emphasis>exposure</emphasis></link>, while pre-applied gamma is part of
      the sRGB standard and in most cases implicitly expected of any JPEG that comes without
      colorspace information.
    </para>

    <para>
      Some modules can only work on certain representations, and changing it comes at a cost (time
      and quality).  Since quality image processing is computationally expensive, darktable authors
      had an incentive to provide maximum quality and flexibility in processing while minimizing the
      number of image representation switches.
    </para>

    <para>
      Examples of "easy" reasons:
      <itemizedlist>

	<listitem><para>
	  Some modules don't operate on same data, so they couldn't be swapped.
	</para></listitem>
	
	<listitem><para> Some orders would make debatable sense, for example apply <link
	linkend="whitebalance"><emphasis>white balance</emphasis></link> after non-linear
	transformations like <link linkend="base_curve"><emphasis>base curve</emphasis></link> or
	<link linkend="tone_curve"> <emphasis>tone curve</emphasis></link>.  Or remove <link
	linkend="hotpixels"><emphasis>hot pixels</emphasis></link> after <link
	linkend="demosaic"><emphasis>demosaicing</emphasis></link>.  (Indeed those modules are offered
	in non-raw processing as well but their functionality and relevance is limited.)
	</para></listitem>

      </itemizedlist>
    </para>
    
    <para>
      The semantic of the image data changes along the pipeline. Changing the order would break
      modules in various way, from downright inapplicable to subtle inaccuracies that people may
      report as bugs (raw processing assumed below for simplicity):
      <itemizedlist>
	
	<listitem><para>
	  Before <link linkend="demosaic"><emphasis>demosaic</emphasis></link>, image data is
	  (generally) one-plane Bayer-encoded, while after that it's three-plane and linear.
	</para></listitem>
	
	<listitem><para> 

	  An outstanding example of order-dependent processing is <link
	  linkend="denoise_profiled"><emphasis>profiled denoise</emphasis></link>.  It needs a
	  three-plane representation and its operation is bound to a sensor-specific profile.  So it
	  must come after demosaicing but before any module that changes lightness level, like <link
	  linkend="tonemapping"><emphasis>tonemapping</emphasis></link>, <link
	  linkend="exposure"><emphasis>exposure</emphasis></link> or <link
	  linkend="base_curve"><emphasis>base curve</emphasis></link>.  
	
	</para></listitem>
	
	<listitem><para> 
	  <link linkend="input_color_profile"><emphasis>input color profile</emphasis></link>
	  further brings the data to some "generic" (linear Lab) representation, independent of
	  camera details, allowing to apply generic filters with generic parameters (that's
	  important for shareable, reusable styles).
	</para></listitem>

      </itemizedlist>
    </para>
    
    <para>
      The actual order of the modules is not manually chosen by the developers but computed by a
      script in the source tree, named <quote>tools/iop_dependencies.py</quote>, which automatically
      adjusts the module priority in the source code of each module.  At the time this text is
      written, it figures out the order from 200 inter-modules dependencies.
    </para>

  </sect2>

</sect1>
